%!TEX TS-program = xelatex
% !TEX root = ../thesis.tex
% Do not delete; used to set build system

The rise of high-throughput scientific experimentation and data collection has introduced new classes of statistical and computational challenges.
The technologies driving this data explosion are subject to complex new forms of measurement error, requiring sophisticated statistical approaches.
Simultaneously, statistical computing must adapt to larger volumes of data and new computational environments, particularly parallel and distributed settings.
This dissertation presents several computational and theoretical contributions to these challenges.

In chapter 1, we consider the problem of estimating the genome-wide distribution of nucleosome positions from paired-end sequencing data.
We develop a modeling approach based on nonparametric templates that controls for variability due to enzymatic digestion.
We use this to construct a calibrated Bayesian method to detect local concentrations of nucleosome positions.
Inference is carried out via a distributed HMC algorithm that scales linearly in complexity with the length of the genome being analyzed.
We provide MPI-based implementations of the proposed methods, stand-alone and on Amazon EC2, which can provide inferences on an entire S. cerevisiae genome in less than 1 hour on EC2.

We then present a method for absolute quantitation from LC-MS/MS proteomics experiments in chapter 2.
We present a Bayesian model for the non-ignorable missing data mechanism induced by this technology, which includes an unusual combination of censoring and truncation.
We provide a scalable MCMC sampler for inference in this setting, enabling full-proteome analyses using cluster computing environments.
A set of simulation studies and actual experiments demonstrate this approach's validity and utility.

We close in chapter 3 by proposing a theoretical framework for the analysis of preprocessing under the banner of multiphase inference.
Preprocessing forms an oft-neglected foundation for a wide range of statistical and scientific analyses.
We provide some initial theoretical foundations for this area, including distributed preprocessing, building upon previous work in multiple imputation.
We demonstrate that multiphase inferences can, in some cases, even surpass standard single-phase estimators in efficiency and robustness.
Our work suggests several rich paths for further research into the statistical principles underlying preprocessing.
